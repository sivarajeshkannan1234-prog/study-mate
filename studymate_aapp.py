# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QXl3zfN64da55JWnhfcOf8UtIvmfRLg2
"""



"""# Task
Create a Streamlit application called "studymate" that acts as an AI-powered PDF Q&A maker for students. The application should use Python, Streamlit, PyMuPDF for PDF processing, and the "watsonx model instruct 8b" from Hugging Face for question answering.

## Set up the environment

### Subtask:
Install the necessary libraries including `streamlit`, `pymupdf`, `transformers`, and potentially other libraries for interacting with the Watsonx model.

**Reasoning**:
The subtask is to install the necessary libraries. I will use pip to install streamlit, pymupdf, and transformers in a single code block. I don't have specific instructions for accessing the watsonx model at this point, so I will not install additional libraries for that purpose.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit pymupdf transformers

"""## Load the watsonx model

### Subtask:
Load the specified Watsonx model (instruct 8b from Hugging Face) using the `transformers` library. This might involve authentication or specific API calls depending on how the model is hosted or accessed.

**Reasoning**:
Import the necessary classes from the `transformers` library and load the tokenizer and model for the specified Watsonx model.
"""

from transformers import AutoModelForCausalLM, AutoTokenizer

# Specify the model name or path for the "watsonx model instruct 8b"
# NOTE: Replace 'ibm-granite/granite-8b-instruct' with the actual model identifier
# if it differs on Hugging Face. Also, consider authentication if required.
model_name = "ibm-granite/granite-8b-instruct"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load the model
# NOTE: Authentication or specific configurations might be needed depending on how
# this model is accessed. Add placeholder for authentication if required.
# Example placeholder for authentication (replace with actual implementation):
# from huggingface_hub import login
# login("YOUR_HUGGING_FACE_TOKEN")
model = AutoModelForCausalLM.from_pretrained(model_name)

print(f"Tokenizer loaded: {tokenizer}")
print(f"Model loaded: {model}")

"""**Reasoning**:
The previous attempt to load the model failed because the model "ibm-granite/granite-8b-instruct" was not found on Hugging Face or requires authentication. As per the error message and the prompt's instruction to consider authentication, I will add a placeholder for authentication using `huggingface_hub` and retry loading the model. I will also explicitly set `trust_remote_code=True` as some models require this.


"""

from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoTokenizer

# Specify the model name or path for the "watsonx model instruct 8b"
# NOTE: Replace 'ibm-granite/granite-8b-instruct' with the actual model identifier
# if it differs on Hugging Face. Also, consider authentication if required.
model_name = "ibm-granite/granite-8b-instruct"

# Placeholder for authentication - replace with your actual Hugging Face token
# login("YOUR_HUGGING_FACE_TOKEN")

# Load the tokenizer
# Adding trust_remote_code=True as some models require it
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# Load the model
# Adding trust_remote_code=True and a placeholder for authentication if required
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)

print(f"Tokenizer loaded: {tokenizer}")
print(f"Model loaded: {model}")

"""## Load the watsonx model retry 1

### Subtask:
Load the specified Watsonx model (instruct 8b from Hugging Face) using the `transformers` library. This might involve authentication or specific API calls depending on how the model is hosted or accessed.

**Reasoning**:
Load the tokenizer and model for "gpt2" using the `transformers` library as a publicly available alternative.
"""

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the tokenizer for the "gpt2" model
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Load the model for "gpt2"
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Print the loaded tokenizer and model
print(f"Tokenizer loaded: {tokenizer}")
print(f"Model loaded: {model}")

"""## Create a pdf processing function

### Subtask:
Develop a Python function using PyMuPDF to extract text content from a PDF file.

**Reasoning**:
Define a Python function to extract text from a PDF file using PyMuPDF, including error handling.
"""

import fitz # PyMuPDF

def extract_text_from_pdf(file_path):
    """
    Extracts text content from a PDF file.

    Args:
        file_path (str): The path to the PDF file.

    Returns:
        str: The concatenated text content from all pages of the PDF,
             or None if an error occurs.
    """
    try:
        # Check if the file path exists
        try:
            doc = fitz.open(file_path)
        except fitz.FileNotFoundError:
            print(f"Error: File not found at {file_path}")
            return None
        except Exception as e:
            print(f"Error opening file: {e}")
            return None

        text = ""
        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            text += page.get_text()

        doc.close()
        return text

    except Exception as e:
        print(f"An error occurred during text extraction: {e}")
        return None

# Example usage (for testing purposes, a dummy file path is used)
# In a real application, this would be a path to an actual PDF
# dummy_file_path = "dummy.pdf"
# extracted_text = extract_text_from_pdf(dummy_file_path)
# if extracted_text:
#     print("Text extracted successfully (or attempted extraction).")
# else:
#     print("Text extraction failed.")

"""## Build the streamlit application

### Subtask:
Create the basic structure of the Streamlit app, including file upload functionality for PDFs and an input area for questions.

**Reasoning**:
Create the basic structure of the Streamlit app, including file upload functionality for PDFs and an input area for questions.
"""

import streamlit as st

# 2. Set the title of the Streamlit application
st.title("studymate - AI-Powered PDF Q&A")

# 3. Add a file uploader widget
uploaded_file = st.file_uploader("Upload a PDF file", type=["pdf"])

# 4. Add a text input area for the user's question
question = st.text_input("Enter your question about the PDF content:")

"""## Integrate pdf processing and model

### Subtask:
Connect the PDF processing function to the Streamlit app to get the text content. Pass the text content and the user's question to the loaded Watsonx model to generate an answer.

**Reasoning**:
Add a conditional block to the Streamlit app code that executes when a file is uploaded and a question is entered. Inside this block, extract text from the uploaded PDF, prepare the input for the loaded model by formatting the text and question, and then use the model and tokenizer to generate an answer.
"""

import streamlit as st
import fitz # PyMuPDF
from transformers import AutoModelForCausalLM, AutoTokenizer
import io

# Assume model and tokenizer are already loaded from previous steps
# If not, uncomment and run the loading code:
# model_name = "gpt2" # Using gpt2 as a placeholder
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForCausalLM.from_pretrained(model_name)

def extract_text_from_pdf(file):
    """
    Extracts text content from a PDF file-like object.

    Args:
        file (io.BytesIO): The file-like object of the PDF.

    Returns:
        str: The concatenated text content from all pages of the PDF,
             or None if an error occurs.
    """
    try:
        # Open the document from the file-like object
        doc = fitz.open(stream=file.read(), filetype="pdf")
        text = ""
        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            text += page.get_text()
        doc.close()
        return text
    except Exception as e:
        st.error(f"An error occurred during text extraction: {e}")
        return None

st.title("studymate - AI-Powered PDF Q&A")

uploaded_file = st.file_uploader("Upload a PDF file", type=["pdf"])
question = st.text_input("Enter your question about the PDF content:")

# 1. Conditional block for processing
if uploaded_file is not None and question:
    # 2. Extract text from the uploaded PDF
    # Streamlit's uploaded_file is a BytesIO object
    pdf_text = extract_text_from_pdf(uploaded_file)

    if pdf_text:
        # 3. Prepare the input for the loaded model (using gpt2 format as placeholder)
        # For gpt2, a simple concatenation might work, or a more structured prompt
        # depending on how it was fine-tuned. Let's use a simple format.
        # A more sophisticated model like instruct 8b would have a specific prompt format.
        # Example prompt format for instruction-following models:
        # "Context: [extracted_text]\nQuestion: [user_question]\nAnswer:"
        prompt = f"Context: {pdf_text}\nQuestion: {question}\nAnswer:"

        try:
            # 4. Use the loaded model and tokenizer to generate an answer
            # Tokenize the input
            # Add padding token if not present
            if tokenizer.pad_token is None:
                 tokenizer.pad_token = tokenizer.eos_token

            inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512) # Adjust max_length as needed

            # Generate the output
            # Using max_length for generation to avoid infinite loops and control output length
            # Adjust num_beams for beam search if desired
            output = model.generate(inputs["input_ids"], attention_mask=inputs["attention_mask"], max_length=inputs["input_ids"].shape[1] + 100, num_beams=5, early_stopping=True) # Generate up to 100 new tokens

            # Decode the output
            # Decode only the newly generated tokens, excluding the prompt
            answer = tokenizer.decode(output[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)

            # 5. Store the generated answer
            generated_answer = answer

            # Display the answer (optional, but helpful for verification)
            st.subheader("Generated Answer:")
            st.write(generated_answer)

        except Exception as e:
            st.error(f"An error occurred during model inference: {e}")
            generated_answer = None
    else:
        generated_answer = None # No text extracted

# The generated_answer variable now holds the model's response,
# which will be used in the next step for display.

"""## Display the answer

### Subtask:
Show the generated answer in the Streamlit application.

**Reasoning**:
Continue with the Streamlit application code to display the generated answer after it has been computed and stored in the `generated_answer` variable.
"""

# The code from the previous step already includes the display logic:
# st.subheader("Generated Answer:")
# st.write(generated_answer)

# This part of the code is already integrated into the previous step's solution.
# No new code is required for this specific subtask as it was completed in the last step.

"""## Refine and improve

### Subtask:
Add features like error handling, progress indicators, and potentially more advanced question answering techniques or model fine-tuning.

**Reasoning**:
Implement the requested enhancements to the Streamlit app, including adding spinners, more specific error handling for model inference, messages for missing inputs, and a sidebar for potential future options.
"""

import streamlit as st
import fitz # PyMuPDF
from transformers import AutoModelForCausalLM, AutoTokenizer
import io

# Assume model and tokenizer are already loaded from previous steps
# If not, uncomment and run the loading code:
# model_name = "gpt2" # Using gpt2 as a placeholder
# try:
#     tokenizer = AutoTokenizer.from_pretrained(model_name)
#     model = AutoModelForCausalLM.from_pretrained(model_name)
# except Exception as e:
#     st.error(f"Error loading model or tokenizer: {e}")
#     st.stop() # Stop the app if the model cannot be loaded

# Placeholder for loaded model and tokenizer if not loaded in previous cells
# In a real app, these would be loaded once outside the main app logic
# For demonstration, we'll assume they are available globally or passed
# For the sake of running this code block independently, we will load gpt2
try:
    model_name = "gpt2" # Using gpt2 as a placeholder
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
except Exception as e:
    st.error(f"Error loading model or tokenizer: {e}")
    st.stop() # Stop the app if the model cannot be loaded


def extract_text_from_pdf(file):
    """
    Extracts text content from a PDF file-like object.

    Args:
        file (io.BytesIO): The file-like object of the PDF.

    Returns:
        str: The concatenated text content from all pages of the PDF,
             or None if an error occurs.
    """
    try:
        # Open the document from the file-like object
        doc = fitz.open(stream=file.read(), filetype="pdf")
        text = ""
        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            text += page.get_text()
        doc.close()
        return text
    except Exception as e:
        st.error(f"An error occurred during text extraction: {e}")
        return None

st.title("studymate - AI-Powered PDF Q&A")

# Add a sidebar for potential options
st.sidebar.header("Options")
# Example of adding an option (not fully implemented in this step)
# model_option = st.sidebar.selectbox("Select Model", ["gpt2", "Another Model"])


uploaded_file = st.file_uploader("Upload a PDF file", type=["pdf"])
question = st.text_input("Enter your question about the PDF content:")

# Add a button to trigger the process
if st.button("Get Answer"):
    # 3. Add a clear message if no PDF or question
    if uploaded_file is None:
        st.warning("Please upload a PDF file.")
    elif not question:
        st.warning("Please enter a question.")
    else:
        # 1. Add st.spinner() around processing steps
        with st.spinner("Processing PDF and generating answer..."):
            # 2. Extract text from the uploaded PDF
            pdf_text = extract_text_from_pdf(uploaded_file)

            if pdf_text:
                # 3. Prepare the input for the loaded model (using gpt2 format as placeholder)
                prompt = f"Context: {pdf_text}\nQuestion: {question}\nAnswer:"

                try:
                    # 4. Use the loaded model and tokenizer to generate an answer
                    if tokenizer.pad_token is None:
                         tokenizer.pad_token = tokenizer.eos_token

                    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512)

                    output = model.generate(inputs["input_ids"], attention_mask=inputs["attention_mask"], max_length=inputs["input_ids"].shape[1] + 100, num_beams=5, early_stopping=True)

                    answer = tokenizer.decode(output[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)

                    # 2. Implement more specific error handling for model inference
                    if not answer or answer.strip() == "":
                         st.warning("The model did not generate a clear answer. Please try rephrasing your question.")
                    else:
                        # 5. Store and display the generated answer
                        st.subheader("Generated Answer:")
                        st.write(answer)

                except Exception as e:
                    st.error(f"An error occurred during model inference: {e}")
            # No need for an else here for pdf_text, as extract_text_from_pdf
            # already displays an error if extraction fails.

"""## Summary:

### Data Analysis Key Findings

*   The initial attempt to load the "watsonx model instruct 8b" from Hugging Face failed due to potential authentication requirements or an incorrect model identifier, resulting in `OSError` and `HTTPError 401 Client Error: Unauthorized`.
*   A publicly available model, "gpt2", was successfully loaded as a placeholder for demonstration purposes using `transformers`.
*   A Python function `extract_text_from_pdf` was successfully created using PyMuPDF (`fitz`) to extract text from PDF file-like objects, including basic error handling.
*   The basic structure of a Streamlit application was built, featuring a title, a PDF file uploader, and a text input for user questions.
*   The PDF text extraction function and the loaded "gpt2" model inference were integrated into the Streamlit application flow, allowing text to be extracted from uploaded PDFs and used, along with the user's question, to generate an answer using the model.
*   The generated answer is displayed in the Streamlit application under a "Generated Answer:" subheader.
*   The application was refined to include `st.spinner()` for progress indication during processing, error handling for missing inputs (PDF or question) and potential issues during model inference, and a sidebar for future expansion.

### Insights or Next Steps

*   To fulfill the original requirement of using the "watsonx model instruct 8b", proper authentication credentials and the correct model identifier for accessing the model on Hugging Face or via an alternative method (like the Watsonx API) are necessary.
*   Implement more sophisticated prompt engineering techniques or fine-tuning of the chosen language model to improve the quality and relevance of the generated answers based on the PDF content.

"""
